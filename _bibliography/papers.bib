---
---
@article{emrullah2024ccmc,
title={From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers},
author={Ildiz, M. Emrullah and Huang, Yixiao and Li, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
journal={arXiv preprint arXiv:2402.13512},
year={Preprint},
arxiv={2402.13512},
abbr={ArXiv},
selected={true}
}

@article{yixiao2023ntg,
title={Mechanics of Next Token Prediction with Self-Attention},
author={Li*, Yingcong and Huang*, Yixiao and Ildiz, M. Emrullah and Rawat, Ankit Singh and Oymak, Samet},
journal={Accepted by International Conference on Artificial Intelligence and Statistics (AISTATS 2024)},
year={2024},
abstract={Transformer-based language models are trained on large datasets to predict the next token
given an input sequence. Despite this simple training objective, they have led to revolutionary
advances in natural language processing. Underlying this success is the self-attention mechanism.
In this work, we ask: What does a single self-attention layer learn from next-token prediction?
We show that training self-attention with gradient descent learns an automaton which generates
the next token in two distinct steps: (1) Hard retrieval: Given input sequence, self-attention
precisely selects the high-priority input tokens associated with the last input token. (2) Soft
composition: It then creates a convex combination of the high-priority tokens from which next
token can be sampled. Under suitable conditions, we rigorously characterize these mechanics
through a graph of token interactions extracted from the training data and show that self-attention
learns to retrieve the tokens that belong to the high-priority strongly-connected components of
this graph. Our 