---
---
@article{emrullah2024ccmc,
title={From Self-Attention to Markov Models: Unveiling the Dynamics of Generative Transformers},
author={Ildiz, M. Emrullah and Huang, Yixiao and Li*, Yingcong and Rawat, Ankit Singh and Oymak, Samet},
journal={arXiv preprint	arXiv:2402.13512},
year={Preprints},
arxiv={2402.13512},
abstract={Modern language models rely on the transformer architecture and attention mechanism to perform language understanding and text generation. In this work, we study learning a 1-layer self-attention model from a set of prompts and associated output data sampled from the model. We first establish a precise mapping between the self-attention mechanism and Markov models: Inputting a prompt to the model samples the output token according to a context-conditioned Markov chain (CCMC) which weights the transition matrix of a base Markov chain. Additionally, incorporating positional encoding results in position-dependent scaling of the transition probabilities. Building on this formalism, we develop identifiability/converage conditions for the prompt distribution that guarantee consistent estimation and establish sample complexity guarantees under IID samples. Finally, we study the problem of learning from a single output trajectory generated from an initial prompt. We characterize an intriguing winner-takes-all phenomenon where the generative process implemented by self-attention collapses into sampling a limited subset of tokens due to its non-mixing nature. This provides a mathematical explanation to the tendency of modern LLMs to generate repetitive text. In summary, the equivalence to CCMC provides a simple but powerful framework to study self-attention and its properties.},
abbr={ArXiv},
selected={true}
}
@article{yixiao2023ntg,
title={Mechanics of Next Token Prediction with Self-Attention},
author={Li*, Yingcong and Huang*, Yixiao and Ildiz, M. Emrullah and Rawat, Ankit Singh and Oymak, Samet},
journal={Accepted by International Conference on Artificial Intelligence and Statistics (AISTATS 2024)},
year={2024},
url={},
pdf={},
abbr={AISTATS 2024},
selected={true}
}

@article{yixiao2023pgd,
title={Sparse-{PGD}: An Effective and Efficient Attack for \$l\_0\$ Bounded Adversarial Perturbation},
author={Zhong, Xuyang and Huang, Yixiao and Liu, Chen},
journal={In submission},
year={2023},
pdf={https://openreview.net/forum?id=BtmB8WrPSp},
url={},
abbr={In submission},
selected={true},
}

@article{liu2022dynamic,
title={Dynamic Statistical Learning with Engineered Features Outperforms Deep Neural Networks for Smart Building Cooling Load Predictions},
author={Yiren Liu and S. Joe Qin and Xiangyu Zhao and Yixiao HUANG and Shenglong Yao and Guo Han},
journal={I Can't Believe It's Not Better Workshop: Understanding Deep Learning Through Empirical Falsification},
year={2022},
abbr={ICBINB@NeurIPS},
pdf={https://openreview.net/forum?id=lKO0OqKn92},
url={https://openreview.net/forum?id=lKO0OqKn92}
}

@article{yixiao2022als,
  author={Huang, Yixiao and Wu, Xiaoli and Chan, Rosa H. M. and Pooled Resource Open-Access ALS Clinical Trials Consortium},
  journal={2022 IEEE-EMBS International Conference on Biomedical and Health Informatics (BHI)}, 
  title={Stratification and Survival Prediction for Amyotrophic Lateral Sclerosis Patients}, 
  year={2022},
  volume={},
  number={},
  pdf={https://ieeexplore.ieee.org/document/9926946},
  abbr={BHI},
  pages={1-5},
  doi={10.1109/BHI56158.2022.9926946},
  selected={false}
}